{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "\n",
    "Here we will mainly focus on building off of `03_imputation_clipped_mean.csv` by doing the following:\n",
    "\n",
    "1. reviewing columns for any logical clipping of obviously wrong values\n",
    "2. skewed distribution handling\n",
    "3. scaling/normalization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review feature distributions (using data wrangler)\n",
    "\n",
    "train_dataset = pd.read_csv(\"../data/03_imputation_clipped_mean.csv\")\n",
    "train_dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                            float64\n",
       "Annual Income                  float64\n",
       "Number of Dependents           float64\n",
       "Health Score                   float64\n",
       "Previous Claims                float64\n",
       "Vehicle Age                    float64\n",
       "Credit Score                   float64\n",
       "Insurance Duration             float64\n",
       "Premium Amount                 float64\n",
       "Policy Duration Mins           float64\n",
       "Gender_Male                       bool\n",
       "Marital Status_Married            bool\n",
       "Marital Status_Single             bool\n",
       "Education Level_High School       bool\n",
       "Education Level_Master's          bool\n",
       "Education Level_PhD               bool\n",
       "Occupation_Self-Employed          bool\n",
       "Occupation_Unemployed             bool\n",
       "Location_Suburban                 bool\n",
       "Location_Urban                    bool\n",
       "Policy Type_Comprehensive         bool\n",
       "Policy Type_Premium               bool\n",
       "Customer Feedback_Good            bool\n",
       "Customer Feedback_Poor            bool\n",
       "Smoking Status_Yes                bool\n",
       "Exercise Frequency_Monthly        bool\n",
       "Exercise Frequency_Rarely         bool\n",
       "Exercise Frequency_Weekly         bool\n",
       "Property Type_Condo               bool\n",
       "Property Type_House               bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing for linear models\n",
    "\n",
    "transform y\n",
    "\n",
    "apply standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handle Skewed Distributions\n",
    "Why: Many machine learning models (especially linear ones) assume features are normally distributed.\n",
    "\n",
    "How:\n",
    "\n",
    "Log Transformation: For highly positive-skewed data (e.g., income).\n",
    "\n",
    "Box-Cox Transformation: For reducing skewness in a broader range of distributions (requires data > 0).\n",
    "\n",
    "Winsorization: Replace extreme values with the nearest threshold instead of removing them.\n",
    "Example:\n",
    "\n",
    "Column: Salary\n",
    "Data: [25k, 30k, 28k, 1.2M, 29k, 31k]\n",
    "Action: Apply log transformation.\n",
    "Result: [10.12, 10.31, 10.24, 13.00, 10.27, 10.34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform_cols = ['Annual Income','Health Score', ]\n",
    "reverse_log_transform_cols = []\n",
    "\n",
    "#unsure: previous claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling/Normalization/Standardization\n",
    "Why: Helps models converge faster and prevents one feature from dominating due to its scale.\n",
    "\n",
    "Methods:\n",
    "\n",
    "Normalization: Scale to a fixed range [0, 1].\n",
    "\n",
    "Standardization: Center data around 0 with unit variance.\n",
    "\n",
    "Robust Scaling: Scale using the median and IQR (useful for data with outliers).\n",
    "\n",
    "Example:\n",
    "\n",
    "Column: Height\n",
    "\n",
    "Data: [150, 160, 170, 180]\n",
    "\n",
    "Action: Normalize.\n",
    "\n",
    "Result: [0, 0.33, 0.67, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Standardizing features (using z-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normalizing features (scaling between [0,1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for tree-based models\n",
    "\n",
    "don't transform y\n",
    "\n",
    "apply nomalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
