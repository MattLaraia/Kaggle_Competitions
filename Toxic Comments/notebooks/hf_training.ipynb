{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: huggingface transformers\n",
    "\n",
    "After doing some research, it is clear that we can also use huggingface to train a model, outputting the probabilities and then performing auc-roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons learned\n",
    "\n",
    "Found that the model was training slow when training with a greater batch size than 8 despite knowing that I had used batch size 32 and 64 on this same machine with a similiar dataset size.  \n",
    "\n",
    "What I failed to initially realize was that the tokenized data was of a larger token max_length than the previous dataset.  The other set had max_length padding of size 128 while here I was using 512 since the model was bert-base-uncased.  Since the padding was 4x larger, the batch size had to be 4x smaller to fit into the GPU memory. \n",
    "\n",
    "# Issues\n",
    "The subtrain/subval split is not using the same split because i stratify by each of the individual labels. This is not the same as stratifying by the combination of labels, giving different data to be used on the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUN=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation of data preprocessing\n",
    "\n",
    "One of the biggest things I want to find out is how small I can make the tokenizer truncate the data to without it impacting a majority of the samples.  Since 95% of the data is about 230 words, I will be using 256 for the max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv files\n",
    "train = pd.read_csv('../data/train_split.csv')\n",
    "valid = pd.read_csv('../data/valid_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['word_count'] = train['comment_text'].apply(lambda x: len(str(x).split()))\n",
    "#get the 95% percentile\n",
    "train['word_count'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid['word_count'] = valid['comment_text'].apply(lambda x: len(str(x).split()))\n",
    "#get the 95% percentile\n",
    "valid['word_count'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic',\t'severe_toxic'\t,'obscene',\t'threat',\t'insult', \t'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dfs_train = {}\n",
    "for label in label_cols:\n",
    "    train_copy = train.copy()\n",
    "    #stratify binary_dfs_train['toxic'] with train as 3% of the data\n",
    "    subtrain, _ = train_test_split(train_copy, test_size=0.97, random_state=42, stratify=train_copy[label], )\n",
    "    binary_dfs_train[label] = subtrain.copy()\n",
    "    binary_dfs_train[label]['label_name'] = binary_dfs_train[label][label].apply(lambda x: label if x==1 else 'other')\n",
    "    binary_dfs_train[label] = binary_dfs_train[label][['id', 'comment_text', label]]\n",
    "    #rename the label column to 'label'\n",
    "    binary_dfs_train[label].rename(columns={label:'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dfs_val = {}\n",
    "for label in label_cols:\n",
    "    val_copy = valid.copy()\n",
    "    #stratify binary_dfs_val['toxic'] with 10% of the data\n",
    "    subvalid,_ = train_test_split(val_copy, test_size=0.9, random_state=42, stratify=val_copy[label] )\n",
    "    binary_dfs_val[label] = subvalid.copy()\n",
    "    binary_dfs_val[label]['label_name'] = binary_dfs_val[label][label].apply(lambda x: label if x==1 else 'other')\n",
    "    binary_dfs_val[label] = binary_dfs_val[label][['id', 'comment_text', label]]\n",
    "    #rename the label column to 'label'\n",
    "    binary_dfs_val[label].rename(columns={label:'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#train a huggingface classifier\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if cuda is available\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B disabled.\n"
     ]
    }
   ],
   "source": [
    "!wandb disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3829 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 7810.02 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 7196.60 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 8379.41 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 7490.83 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 7656.83 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 6973.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "binary_datasets_train = {}\n",
    "for key in binary_dfs_train.keys():\n",
    "    \n",
    "    #convert the data into a Dataset object\n",
    "    binary_datasets_train[key] = datasets.Dataset.from_pandas(binary_dfs_train[key])\n",
    "    binary_datasets_train[key] = binary_datasets_train[key].map(lambda batch: tokenizer(batch['comment_text'], truncation=True, padding='max_length',max_length=256), batched=True,batch_size=64)\n",
    "    \n",
    "    binary_datasets_train[key].set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3191 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7684.39 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7348.92 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 8035.14 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7015.58 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7988.60 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7112.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "binary_datasets_val = {}\n",
    "for key in binary_dfs_val.keys():\n",
    "\n",
    "\n",
    "    #convert the data into a Dataset object\n",
    "    binary_datasets_val[key] = datasets.Dataset.from_pandas(binary_dfs_val[key])\n",
    "    binary_datasets_val[key] = binary_datasets_val[key].map(lambda batch: tokenizer(batch['comment_text'], truncation=True, padding='max_length',max_length=256), batched=True,batch_size=64)\n",
    "    \n",
    "    binary_datasets_val[key].set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "                                                 \n",
      " 33%|███▎      | 120/360 [00:23<00:35,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12240316718816757, 'eval_runtime': 3.8642, 'eval_samples_per_second': 825.783, 'eval_steps_per_second': 12.939, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 229/360 [00:42<00:22,  5.81it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mlaraia\\extra_projects\\Kaggle_Competitions\\Toxic Comments\\notebooks\\hf_training.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,                         \u001b[39m# the instantiated 🤗 Transformers model to be trained\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,                  \u001b[39m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mbinary_datasets_train[key],         \u001b[39m# training dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mbinary_datasets_val[key],             \u001b[39m# evaluation dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#evaluate the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\transformers\\trainer.py:1815\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1812\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1815\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1816\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1817\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mc:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\accelerate\\data_loader.py:393\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[39m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 393\u001b[0m         current_batch \u001b[39m=\u001b[39m send_to_device(current_batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    394\u001b[0m     next_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m batch_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_batches:\n",
      "File \u001b[1;32mc:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\accelerate\\utils\\operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[1;32m--> 160\u001b[0m         {\n\u001b[0;32m    161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39mnon_blocking, skip_keys\u001b[39m=\u001b[39mskip_keys)\n\u001b[0;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    163\u001b[0m         }\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\accelerate\\utils\\operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[0;32m    160\u001b[0m         {\n\u001b[1;32m--> 161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[0;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    163\u001b[0m         }\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\accelerate\\utils\\operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mto(device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking)\n\u001b[0;32m    168\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import trainer and training arguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "for key in binary_datasets_train.keys():\n",
    "    #instantiate the model and use the label list to define the labels\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    #instantiate the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'../results/{key}/{RUN}',          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=64,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "        learning_rate=5e-05,             # learning rate\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    #instantiate the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=binary_datasets_train[key],         # training dataset\n",
    "        eval_dataset=binary_datasets_val[key],             # evaluation dataset\n",
    "    )   \n",
    "\n",
    "    #train the model\n",
    "    trainer.train()\n",
    "\n",
    "    #evaluate the model\n",
    "    trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
