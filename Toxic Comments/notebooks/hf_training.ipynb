{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: huggingface transformers\n",
    "\n",
    "After doing some research, it is clear that we can also use huggingface to train a model, outputting the probabilities and then performing auc-roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons learned\n",
    "\n",
    "Found that the model was training slow when training with a greater batch size than 8 despite knowing that I had used batch size 32 and 64 on this same machine with a similiar dataset size.  \n",
    "\n",
    "What I failed to initially realize was that the tokenized data was of a larger token max_length than the previous dataset.  The other set had max_length padding of size 128 while here I was using 512 since the model was bert-base-uncased.  Since the padding was 4x larger, the batch size had to be 4x smaller to fit into the GPU memory. \n",
    "\n",
    "# Issues\n",
    "The subtrain/subval split is not using the same split because i stratify by each of the individual labels. This is not the same as stratifying by the combination of labels, giving different data to be used on the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUN=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation of data preprocessing\n",
    "\n",
    "One of the biggest things I want to find out is how small I can make the tokenizer truncate the data to without it impacting a majority of the samples.  Since 95% of the data is about 230 words, I will be using 256 for the max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv files\n",
    "train = pd.read_csv('../data/train_split.csv')\n",
    "valid = pd.read_csv('../data/valid_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['word_count'] = train['comment_text'].apply(lambda x: len(str(x).split()))\n",
    "#get the 95% percentile\n",
    "train['word_count'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid['word_count'] = valid['comment_text'].apply(lambda x: len(str(x).split()))\n",
    "#get the 95% percentile\n",
    "valid['word_count'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic',\t'severe_toxic'\t,'obscene',\t'threat',\t'insult', \t'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dfs_train = {}\n",
    "for label in label_cols:\n",
    "    train_copy = train.copy()\n",
    "    #stratify binary_dfs_train['toxic'] with train as 3% of the data\n",
    "    subtrain, _ = train_test_split(train_copy, test_size=0.97, random_state=42, stratify=train_copy[label], )\n",
    "    binary_dfs_train[label] = subtrain.copy()\n",
    "    binary_dfs_train[label]['label_name'] = binary_dfs_train[label][label].apply(lambda x: label if x==1 else 'other')\n",
    "    binary_dfs_train[label] = binary_dfs_train[label][['id', 'comment_text', label]]\n",
    "    #rename the label column to 'label'\n",
    "    binary_dfs_train[label].rename(columns={label:'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dfs_val = {}\n",
    "for label in label_cols:\n",
    "    val_copy = valid.copy()\n",
    "    #stratify binary_dfs_val['toxic'] with 10% of the data\n",
    "    subvalid,_ = train_test_split(val_copy, test_size=0.9, random_state=42, stratify=val_copy[label] )\n",
    "    binary_dfs_val[label] = subvalid.copy()\n",
    "    binary_dfs_val[label]['label_name'] = binary_dfs_val[label][label].apply(lambda x: label if x==1 else 'other')\n",
    "    binary_dfs_val[label] = binary_dfs_val[label][['id', 'comment_text', label]]\n",
    "    #rename the label column to 'label'\n",
    "    binary_dfs_val[label].rename(columns={label:'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#train a huggingface classifier\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if cuda is available\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B disabled.\n"
     ]
    }
   ],
   "source": [
    "!wandb disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3829 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3829/3829 [00:00<00:00, 6111.57 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3829/3829 [00:00<00:00, 6240.10 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3829/3829 [00:00<00:00, 6578.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3829/3829 [00:00<00:00, 6270.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3829/3829 [00:00<00:00, 6621.89 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3829/3829 [00:00<00:00, 5605.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "binary_datasets_train = {}\n",
    "for key in binary_dfs_train.keys():\n",
    "    \n",
    "    #convert the data into a Dataset object\n",
    "    binary_datasets_train[key] = datasets.Dataset.from_pandas(binary_dfs_train[key])\n",
    "    binary_datasets_train[key] = binary_datasets_train[key].map(lambda batch: tokenizer(batch['comment_text'], truncation=True, padding='max_length',max_length=256), batched=True,batch_size=64)\n",
    "    \n",
    "    binary_datasets_train[key].set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3191 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3191/3191 [00:00<00:00, 5599.23 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3191/3191 [00:00<00:00, 5320.54 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3191/3191 [00:00<00:00, 4916.20 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3191/3191 [00:00<00:00, 5604.64 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3191/3191 [00:00<00:00, 4781.67 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3191/3191 [00:00<00:00, 4823.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "binary_datasets_val = {}\n",
    "for key in binary_dfs_val.keys():\n",
    "\n",
    "\n",
    "    #convert the data into a Dataset object\n",
    "    binary_datasets_val[key] = datasets.Dataset.from_pandas(binary_dfs_val[key])\n",
    "    binary_datasets_val[key] = binary_datasets_val[key].map(lambda batch: tokenizer(batch['comment_text'], truncation=True, padding='max_length',max_length=256), batched=True,batch_size=64)\n",
    "    \n",
    "    binary_datasets_val[key].set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "                                                 \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 120/360 [00:48<01:09,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12830430269241333, 'eval_runtime': 9.6759, 'eval_samples_per_second': 329.789, 'eval_steps_per_second': 5.167, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 240/360 [01:38<00:36,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1382884383201599, 'eval_runtime': 9.6858, 'eval_samples_per_second': 329.452, 'eval_steps_per_second': 5.162, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [02:20<00:00,  3.22it/s]"
     ]
    }
   ],
   "source": [
    "#import trainer and training arguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "for key in binary_datasets_train.keys():\n",
    "    #instantiate the model and use the label list to define the labels\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    #instantiate the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'../results/{key}/{RUN}',          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=64,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "        learning_rate=5e-05,             # learning rate\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    #instantiate the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=binary_datasets_train[key],         # training dataset\n",
    "        eval_dataset=binary_datasets_val[key],             # evaluation dataset\n",
    "    )   \n",
    "\n",
    "    #train the model\n",
    "    trainer.train()\n",
    "\n",
    "    #evaluate the model\n",
    "    trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
