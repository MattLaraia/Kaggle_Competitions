{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv.zip')\n",
    "\n",
    "#allow all columns to be displayed\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, lets take a closer look at the data. We will look at the distribution of the data as well as the number of missing values and the type of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Data distibution and any bad values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    144277\n",
      "1     15294\n",
      "Name: toxic, dtype: int64\n",
      "0    0.904156\n",
      "1    0.095844\n",
      "Name: toxic, dtype: float64\n",
      "0    157976\n",
      "1      1595\n",
      "Name: severe_toxic, dtype: int64\n",
      "0    0.990004\n",
      "1    0.009996\n",
      "Name: severe_toxic, dtype: float64\n",
      "0    151122\n",
      "1      8449\n",
      "Name: obscene, dtype: int64\n",
      "0    0.947052\n",
      "1    0.052948\n",
      "Name: obscene, dtype: float64\n",
      "0    159093\n",
      "1       478\n",
      "Name: threat, dtype: int64\n",
      "0    0.997004\n",
      "1    0.002996\n",
      "Name: threat, dtype: float64\n",
      "0    151694\n",
      "1      7877\n",
      "Name: insult, dtype: int64\n",
      "0    0.950636\n",
      "1    0.049364\n",
      "Name: insult, dtype: float64\n",
      "0    158166\n",
      "1      1405\n",
      "Name: identity_hate, dtype: int64\n",
      "0    0.991195\n",
      "1    0.008805\n",
      "Name: identity_hate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "toxic_types = ['toxic',\t'severe_toxic'\t,'obscene',\t'threat',\t'insult', \t'identity_hate']\n",
    "\n",
    "#get the value counts for each toxic type\n",
    "for toxic_type in toxic_types:\n",
    "    print(df[toxic_type].value_counts())\n",
    "    print(df[toxic_type].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    143346\n",
      "1      6360\n",
      "3      4209\n",
      "2      3480\n",
      "4      1760\n",
      "5       385\n",
      "6        31\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.898321\n",
       "1    0.039857\n",
       "3    0.026377\n",
       "2    0.021808\n",
       "4    0.011030\n",
       "5    0.002413\n",
       "6    0.000194\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find out how many of the rows have no toxic comments\n",
    "print(df[toxic_types].sum(axis=1).value_counts())\n",
    "df[toxic_types].sum(axis=1).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the majority of the data has been labeled as non-toxic. We may need to be careful about the model overfitting to just label non-toxic. We will need to look at the other labels to see if they are also skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1595, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_toxic=df[(df.toxic==1) & (df.severe_toxic==1)]\n",
    "double_toxic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1595, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severely_toxic = df[df.severe_toxic==1]\n",
    "severely_toxic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_toxic.equals(severely_toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6155, 8)\n",
      "(7877, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8449, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all data with obscene and insult comments\n",
    "obscene_insult = df[(df.obscene==1) & (df.insult==1)]\n",
    "print(obscene_insult.shape)\n",
    "\n",
    "insult = df[df.insult==1]\n",
    "insult.shape\n",
    "print(insult.shape)\n",
    "\n",
    "\n",
    "#get all data with obscene  comments\n",
    "obscene = df[df.obscene==1]\n",
    "obscene.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly all the ones tagged as severe_toxic are also labelled as toxic. Also a lot of the obscene and insult comments are also frequently labelled together.  Perhaps it will be useful to look at some of the examples that are labelled as only one or the other To get a better idea for what defines that category of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at samples within the categories to get a better idea of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1722, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using df, get all data with obscene  comments and insult comments not equal to 1\n",
    "obscene_not_insult = df[(df.obscene==1) & (df.insult!=1)]\n",
    "obscene_not_insult.shape\n",
    "\n",
    "#using df, get all data with insult  comments not equal to 1\n",
    "insult_not_obscene = df[(df.obscene!=1) & (df.insult==1)]\n",
    "insult_not_obscene.shape\n",
    "\n",
    "#using df, get all data with obscene  comments and all other toxic types not equal to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK THAT YOU CANT PUT DOWN???/ GET FUCK UP GET FUCKED UP.  I'M FUCKED UP RIGHT NOW!\n",
      "------------------\n",
      "My Band Page's deletion. You thought I was gone. \n",
      "\n",
      "Deleting the comment I posted on your 'talk page' does not delete my feelings for your hasty decision to delete my page. You, sir, are still a cock-sucking douche fuck. Sit back for a second and think about what your life has become. ............ Done? That didn't take long did it? Nope. Because, as I am most certainly aware, your life is a shitstorm of sitting in front of the computer masturbating to fictional creatures. Your attempts to get rid of me are mediocre at best. You are constantly sucking the dick of failure. You don't want a personal attack, huh? Well, too bad, Fuckcock McDickerson. You've got one. From me. You can go ahead and delete my profile from wikipedia. I'll just make another one and come right back to 'Syrthiss's talk page and insult the dick off of you. How could you shatter the dreams of an innocent eighteen year old college freshman trying to make a name for his band. Does that make you happy? Fucking with people because you're an overweight, single, old man in a dead-end job. Did you spot that perhaps someone else was going to follow his dreams and you were trying to hold him back so somebody else could suffer like you? Yes you did. I don't make empty threats, so I won't be saying anything along the lines of 'i'll hurt you' or 'i'll eat the children from within your sister's womb', but I will say that you are a asshole, son-of-a-bitch, mother fucking cock sucker. So, go eat some more food and drown your sorrows you premature ejaculating, bald headed fuck.\n",
      "\n",
      "You should do something nice for yourself, maybe go grab a couple of Horny Goat Weeds from your local convenience store and jack off for a little longer than three minutes tonight.\n",
      "\n",
      "Sincerely,\n",
      "An Asshole That's Better Than You In Every Way.\n",
      "------------------\n",
      "Your blatant POV pushing \n",
      "\n",
      "Neither of you guys has made any contribution to this Italian history article other than to shove your unhistorical unconstructive modern POV in my face.\n",
      "This is a HISTORY article. HISTORY.\n",
      "Have you heard of that?\n",
      "This is the reason why so many people get pissed off about the pedantry and idiocy and triviality of Wikipedia.\n",
      "J*sus. Get a f*cking life.\n",
      "------------------\n",
      "Fuck off\n",
      "\n",
      "You are NOT an administrator. You DON'T have the authority to tell me what to do.\n",
      "------------------\n",
      "MATT HARDY IS SO FUCKY!!!''Italic text[[Media:Example.ogg]]'''''MATT HARDY IS SO FUCKY!!!\n",
      "------------------\n",
      "Fucked with the wrong muchacho.  21:19, July 23, 2005 (UTC)\n",
      "------------------\n",
      "HELLO HOW DO I GET SOMEBODY TO FIX SHIT AROUND HERE?\n",
      "------------------\n",
      "How do you know he is dead.  Its just his plane that crashed.  Jeezz, quit busting his nuts, folks.\n",
      "------------------\n",
      "Hahaha, you dont actually believe this bullshit you are telling me right now? You are talking about the access to the country or representatives to the government, not Kim Jong Un himself. Those are two totally different things (although I could understand your stupidity, since North Koreans think that Kim Jong un is the governement, im just saying). Go Troll somewhere else kid.\n",
      "------------------\n",
      "\"You know, I was thinking...\n",
      "\n",
      "Nobody really attacked me before I revealed my age here.  After I announced that I was 14, I was subject to a constant storm of harassment and abuse.\n",
      "\n",
      "The same thing goes for when I was on The Sandbox when I was 11.  Before people knew my age, they didn't care.  After I told them my age, they hated me.\n",
      "\n",
      "It's just so amusing how the idiot Soapboxers think of teenagers as being responsible for the \"\"decline\"\" of The Soapbox (like it was ever any good to begin with).  This is funny, because the adults are the dumbest people here.  Mauricio, Chuppah, Redneckgirl, Pinky, Scooby, GG, ForeverLostTinkerbell, greenrabbit, StarRocketsInFlight, Bree-Robinson, enigma-72, CocaineNightmares, TokyoDome, Chad_Ocho-Cinco, Steve Lake...\n",
      "\n",
      "...all adults.\n",
      "\n",
      "The teenage posters here are typically the smartest ones.  Let's see... G-Dawg, Head_Banging_Brunette, ClassicAge, Metropolis (even though he's a bipolar weirdo), ChrisScript, just to name a few.\n",
      "\n",
      "All I ever see adults posting is \"\"rate my poo!!1!\"\", \"\"my ballz smell funny\"\", \"\"whoz hawter: me or ur mom???//?/?\"\", \"\"i like to rape little girls\"\", and \"\"HEY GUYZ LOOK AT ME IM AN ATTENTION WHORE!!!!11!!1!\"\"  The adult posters here are immature, moronic, savage bullies with the mental capacity of a retarded 4-year-old.\n",
      "\n",
      "Whereas, most of the teenagers here are thoughtful, intelligent, mature, and often profound.  But they're subject to constant schoolyard bullying by the idiot adult posters who think they're somehow superior because they were born (and dropped on their head) earlier.\n",
      "\n",
      "Soapbox logic never fails to amuse me...\n",
      "\"\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "#look at a few of the comments in obscene_not_insult\n",
    "for item in obscene_not_insult.comment_text.head(10):\n",
    "    print(item)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_only shape:  (5666, 8)\n",
      "severe_toxic_only shape:  (0, 8)\n",
      "obscene_only shape:  (317, 8)\n",
      "threat_only shape:  (22, 8)\n",
      "insult_only shape:  (301, 8)\n",
      "identity_hate_only shape:  (54, 8)\n",
      "okay_only shape:  (143346, 8)\n"
     ]
    }
   ],
   "source": [
    "toxic_types = ['toxic',\t'severe_toxic'\t,'obscene',\t'threat',\t'insult', \t'identity_hate']\n",
    "#using df, get all data with obscene  comments and all other toxic types not equal to 1\n",
    "\n",
    "toxic_only = df[(df.toxic==1) & (df[toxic_types].sum(axis=1)==1)]\n",
    "print('toxic_only shape: ', toxic_only.shape)\n",
    "severe_toxic_only = df[(df.severe_toxic==1) & (df[toxic_types].sum(axis=1)==1)]\n",
    "print('severe_toxic_only shape: ', severe_toxic_only.shape)\n",
    "\n",
    "obscene_only = df[(df.obscene==1) & (df[toxic_types].sum(axis=1)==1)]\n",
    "print('obscene_only shape: ', obscene_only.shape)\n",
    "\n",
    "threat_only = df[(df.threat==1) & (df[toxic_types].sum(axis=1)==1)]\n",
    "print('threat_only shape: ', threat_only.shape)\n",
    "\n",
    "insult_only = df[(df.insult==1) & (df[toxic_types].sum(axis=1)==1)]\n",
    "print('insult_only shape: ', insult_only.shape)\n",
    "\n",
    "identity_hate_only = df[(df.identity_hate==1) & (df[toxic_types].sum(axis=1)==1)]\n",
    "print('identity_hate_only shape: ', identity_hate_only.shape)\n",
    "\n",
    "okay_only = df[(df[toxic_types].sum(axis=1)==0)]\n",
    "print('okay_only shape: ', okay_only.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey... what is it..\n",
      "@ | talk .\n",
      "What is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\n",
      "\n",
      "Ask Sityush to clean up his behavior than issue me nonsensical warnings...\n",
      "------------------\n",
      "Bye! \n",
      "\n",
      "Don't look, come or think of comming back! Tosser.\n",
      "------------------\n",
      "I'm Sorry \n",
      "\n",
      "I'm sorry I screwed around with someones talk page.  It was very bad to do.  I know how having the templates on their talk page helps you assert your dominance over them.  I know I should bow down to the almighty administrators.  But then again, I'm going to go play outside....with your mom.   76.122.79.82\n",
      "------------------\n",
      "Why can't you believe how fat Artie is? Did you see him on his recent appearence on the Tonight Show with Jay Leno? He looks absolutely AWFUL! If I had to put money on it, I'd say that Artie Lange is a can't miss candidate for the 2007 Dead pool!   \n",
      "\n",
      "  \n",
      "Kindly keep your malicious fingers off of my above comment, . Everytime you remove it, I will repost it!!!\n",
      "------------------\n",
      "\"\n",
      "\n",
      "SORRY PUCK BUT NO ONE EVER SAID DICK WAS NUMBER ONE!! But he is clearly listed as being \"\"second only to Batman\"\" and a \"\"master detective and martial artist\"\" and been trained in all the mentioned combat discplines. No need to mention in skills/abilities about the battles he's lost and won because thats not what an encylopia is supposed to do. \n",
      "\n",
      "Also, what your problem is your going by low end showings and not HIGH end showings where he's proven more than a match for a meta like Slade and pinned Shiva down in one of his earliest appearences.\n",
      " \"\n",
      "------------------\n",
      "\"\n",
      "\n",
      "UNBLOCK ME OR I'LL GET MY LAWYERS ON TO YOU FOR BLOCKING MY CONSTITUTIONAL RIGHT TO FREE SPEECH\"\n",
      "------------------\n",
      "Give me a permanat block raseac....!!! remember me....!!! Muslims dont you worry for these images they are trying to hurt us but they cant look these images are fake and have nothing to do with Islam or our prophet even their article is so lame wikipedia is not an authentic source its an encyclopedia for the kids which has not authenticity anyone can edit or write whatever they think...just like raeasc you can see he does not knows what Islam is and he talks about it so forget about wiki its lame they have no authenticity...i recommend they should add pissed christ in jessus article....they should...\n",
      "------------------\n",
      "Atheism is full of bias shit\n",
      "------------------\n",
      "Hey why you are spreading misconceptions and trying to spread false information to the people.You not even know who won the battle and who lost.you are the cheapest person on the earth.Bloody ignorant fool.Check sources before spreading rumors\n",
      "------------------\n",
      "\"\n",
      "\n",
      "And you are? Let me know when you've crawled out of their arses. '''''' Dick Laurent is dead \"\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for text in toxic_only.comment_text.head(10):\n",
    "    print(text)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nothing too clear to distinguish about this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you know he is dead.  Its just his plane that crashed.  Jeezz, quit busting his nuts, folks.\n",
      "------------------\n",
      "Bleh. I'm all out of ideas. Is there really something wrong with the server or what? The damn thing won't work for me, either Helvetica_font_new.svg (my upload of Segoe UI was actually a PNG to begin with...)\n",
      "------------------\n",
      "{{unblock|i said im sorry......how do u expect me to stop making accounts if u dont unblock me.....if u unblock me,ill stop making acounts...simple...........ur the ones who are making something bad to worse.....stop making a big deal...im not doing anything bad or violating anything now,so i dont get it,why am i still blocked....just get over it already.......i mean well.....im not here do violate anything.......im just trying to help.......and can someone tell alexandria to unblockme already......this block is so fucking overrated.....cant u just forgive and forget.....move on people....there are other things to worry about than a stupid block...so unblock me and MOVE ON........again,i come in peace......and i have a right to contribute to wikipedia.....what happend in the past happend....the past is in the past.....now were the present......lets open a new page and start a new chapter.....star fresh..ok..deal..??.......i promise i wont violate anything.....all im asking is another chance......one last chance.....\n",
      "------------------\n",
      "\"\n",
      "\n",
      "As for the arguements that abo should be included .. it already is. Wanker is used alot, but only as a common noun, and the usuage isn't unique. Same for the other words mentioned.\n",
      "\n",
      "Though other commonwealth countries (such as new zealand) may use cunt in a similar fashion, Australians do seem to use it more. However, this arguement is irrelevent since many \"\"Australian\"\" words such as \"\"mate, bloke\"\" etc are used extensively in NZ, and the UK, though are still in the article.\n",
      "R0m\"\n",
      "------------------\n",
      "I wish to restate the AN thread as neutral and inviting community discussion. I realize that I went to far by characterizing IH as a dick. I apologize. 140.247.141.165\n",
      "------------------\n",
      "jd2718 the sock fiend \n",
      "\n",
      "now hes lying out his ass on the ani page but I can't comment. and no im not this m1rth person, im just me.\n",
      "------------------\n",
      "Ram it up ur ass very hard till ur eyes water\n",
      "------------------\n",
      "You might try to deflate your ego a bit and stop making blanket generalizations.  The IP I use is a proxy for a university of 20,000 people, plenty of other editors (yes, Macedonians too) who use it. Sometimes I just don't notice I was logged out. Does that satisfy your sensibilities? I haven't called anyone peasant or fuck off, that is your overactive imagination playing tricks on you again.    \n",
      " As for the statement, I am simply presenting the fact that it is much more informative and encyclopedic to say the 'Macedonian National Liberation Army' liberated the city, as they were the only 'communist partisans' operating in Macedonia at that time, which you could have realized yourself by reading the article on MNLA.\n",
      "------------------\n",
      "\"\n",
      "\n",
      " Ahem, wtf are u talking about sire? I do not practice \"\"vandalism\"\".\"\n",
      "------------------\n",
      "\"\n",
      "\n",
      "Yes, the money quote is that I'm fucking pissed off at the insistence of Richard James continually harrassing me, refusing to stop identifying my personal info. (as with posting my personal email above with website). I have vandalized nothing but to take a letter or two off of my personal name and words that lead to my personal info. in the slanderous and libelous and defaming posts of richard james. I don't give a shit that my project is not listed here. I found the whole process interesting and hilarious, though quite hypocritical. WHo can take the piss out of who here. THe article was about an art project, not me. I don't care that it's not there. It doesn't exist, nor does Bigfoot! His compromise was on day two of the original posting of the project article, a one man band who decides fist down who gets what and when, like a fascist pig. I could have campaigned for votes, but what the fuck...Before final vote and deletion, Richard James posted an article about me without my desire nor consent, and then deleted it in a violent fit of rage when I jokingly said the first article is dead at another's insistence that it should be deleted then at seven days. He's had the same hard-on ever since. As I said, I don't give a shit but that his posts are defaming, slanderous, libelous and outright insidious. His high and mighty accusational tone is what is weird and strange. All I have done in the past two days is to delete letters of my name and project, and the old username that I was attempting to expunge. Why?\n",
      "\n",
      "\"\"Such posting can cause offence or embarrassment to the victim of the posting, not least because it means that their name, and any personal criticism or allegations made against them can then appear on web searches. If you have posted such information, please remove it immediately. Please then follow the link to this page and inform people there that the information was posted (but crucially, do not repost it on that page). An admin or developer can then remove the information from the archives of Wikipedia.\"\"\n",
      "\n",
      "\"\"Wikipedia operates on the principle that every contributor has a right if they wish to remain completely anonymous. Wikipedia policy on that issue is strictly enforced. Posting private information about a user, specifically their (alleged) name and/or personal details, is strictly prohibited as harassment, and users who do that are often immediately blocked from editing Wikipedia.\"\" How is it vandalism to attempt to remain anonymous?! and to attempt to separate personal and professional reputation from libelous attacks?\"\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for text in obscene_only.comment_text.head(10):\n",
    "    print(text)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obscene comments seem to include heavily gaslighting, complaining, and blaming others for something they are probably responsible for.  Peoeple that exhibit these behaviors tend to narcissitic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More insights may be made but the time it takes to more closer evaluate the data before geting some sort of a bench mark doesnt not seem helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving onto model training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a lesson learned - multilabel data split\n",
    "\n",
    "A classic stratified split does not work with multilablled data when there are rare multilabeleed instances that only occur once in the dataset.  As a result, I found that rare data and removed it before doing the test/train split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f1d3a0482afc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#split the data into train and validation sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoxic_types\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2469\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m     return list(\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1620\u001b[0m         \"\"\"\n\u001b[0;32m   1621\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1622\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1623\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1966\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1967\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1968\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1969\u001b[0m                 \u001b[1;34m\"The least populated class in y has only 1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m                 \u001b[1;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "\"\"\"#first we need to split the data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the data into train and validation sets\n",
    "train, valid = train_test_split(df, test_size=0.2, random_state=42, stratify=df[toxic_types], )\n",
    "\n",
    "print(train.shape, valid.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a column called multi_label that is the list of all the valid labels column names\n",
    "df['multi_label'] = df[toxic_types].apply(lambda x: x.index[x==1].tolist(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if its an empty list, make it a list with one item, 'non-toxic'\n",
    "df['multi_label'] = df['multi_label'].apply(lambda x: x if len(x)>0 else ['non-toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[non-toxic]                                                      143346\n",
       "[toxic]                                                            5666\n",
       "[toxic, obscene, insult]                                           3800\n",
       "[toxic, obscene]                                                   1758\n",
       "[toxic, insult]                                                    1215\n",
       "[toxic, severe_toxic, obscene, insult]                              989\n",
       "[toxic, obscene, insult, identity_hate]                             618\n",
       "[obscene]                                                           317\n",
       "[insult]                                                            301\n",
       "[toxic, severe_toxic, obscene, insult, identity_hate]               265\n",
       "[obscene, insult]                                                   181\n",
       "[toxic, severe_toxic, obscene]                                      158\n",
       "[toxic, identity_hate]                                              136\n",
       "[toxic, insult, identity_hate]                                      134\n",
       "[toxic, obscene, threat, insult]                                    131\n",
       "[toxic, threat]                                                     113\n",
       "[toxic, severe_toxic, obscene, threat, insult]                       64\n",
       "[toxic, obscene, threat, insult, identity_hate]                      56\n",
       "[identity_hate]                                                      54\n",
       "[toxic, severe_toxic]                                                41\n",
       "[toxic, obscene, identity_hate]                                      35\n",
       "[toxic, severe_toxic, obscene, threat, insult, identity_hate]        31\n",
       "[insult, identity_hate]                                              28\n",
       "[threat]                                                             22\n",
       "[obscene, insult, identity_hate]                                     18\n",
       "[toxic, threat, insult]                                              16\n",
       "[toxic, severe_toxic, insult]                                        14\n",
       "[toxic, obscene, threat]                                             11\n",
       "[toxic, severe_toxic, threat]                                        11\n",
       "[toxic, threat, identity_hate]                                        7\n",
       "[toxic, severe_toxic, insult, identity_hate]                          7\n",
       "[toxic, severe_toxic, obscene, identity_hate]                         6\n",
       "[toxic, severe_toxic, obscene, threat]                                4\n",
       "[threat, insult]                                                      3\n",
       "[toxic, threat, insult, identity_hate]                                3\n",
       "[toxic, severe_toxic, identity_hate]                                  3\n",
       "[obscene, identity_hate]                                              3\n",
       "[obscene, threat]                                                     2\n",
       "[obscene, threat, insult]                                             2\n",
       "[toxic, severe_toxic, threat, insult]                                 1\n",
       "[toxic, severe_toxic, threat, identity_hate]                          1\n",
       "Name: multi_label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.multi_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>multi_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107881</th>\n",
       "      <td>40a358e41dcc21f4</td>\n",
       "      <td>IM GOING TO KILL YOU ALL!!!!!!!!!!!!!!!!!\\nI'm...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[toxic, severe_toxic, threat, insult]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134459</th>\n",
       "      <td>cf012f8122791d7e</td>\n",
       "      <td>DEATH TO ARABS! FREE THE JEWISH HOMELAND OF JU...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[toxic, severe_toxic, threat, identity_hate]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "107881  40a358e41dcc21f4  IM GOING TO KILL YOU ALL!!!!!!!!!!!!!!!!!\\nI'm...   \n",
       "134459  cf012f8122791d7e  DEATH TO ARABS! FREE THE JEWISH HOMELAND OF JU...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "107881      1             1        0       1       1              0   \n",
       "134459      1             1        0       1       0              1   \n",
       "\n",
       "                                         multi_label  \n",
       "107881         [toxic, severe_toxic, threat, insult]  \n",
       "134459  [toxic, severe_toxic, threat, identity_hate]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition1 = (df['toxic'] == 1) & (df['severe_toxic'] == 1) & (df['threat'] == 1) & (df['insult'] == 1)\n",
    "condition2 = (df['toxic'] == 1) & (df['severe_toxic'] == 1) & (df['threat'] == 1) & (df['identity_hate'] == 1)\n",
    "\n",
    "#find all the rows that have include conditions 1 or 2 and do not include any other toxic types\n",
    "df[(condition1 | condition2) & (df[toxic_types].sum(axis=1)==4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_data = df[(condition1 | condition2) & (df[toxic_types].sum(axis=1)==4)]\n",
    "#drop the rare data from the df\n",
    "df = df.drop(rare_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, test_size=0.2, random_state=42, stratify=df['multi_label'], )\n",
    "#add the rare data df to the train set df\n",
    "train = pd.concat([train, rare_data])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "For base evaluation, I will start off using a simpler model\n",
    "\n",
    "Afterwards, I will move onto training a large model, and maybe even a transformer model\n",
    "\n",
    "NOTE:  since the competition is over I could use the labelled test set but I will continue using the validation set to simulate a realistic competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model: Logistic Regression with multiclass using OneVsRestClassifier\n",
    "\n",
    "The classifier appears to be doing well (.979 roc_auc), however when trying to do baseline comparisions, I found that its returning 0.5 no matter which column I want to directly compare against.  This may be indicating some lack of properly formatted data, however I am also not too comfortable with roc_auc.  As a result, I will look into it and try to evaluate a baseling again afterwards\n",
    "\n",
    "Update: a performance of 0.5 in auc_roc means that it is no better than random guessing, which is what I was doing when testing out baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('multi_label', axis=1, inplace=True)\n",
    "valid.drop('multi_label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic',\t'severe_toxic'\t,'obscene',\t'threat',\t'insult', \t'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=0.7, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;, OneVsRestClassifier(estimator=LogisticRegression()))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=0.7, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;, OneVsRestClassifier(estimator=LogisticRegression()))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.7, stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">clf: OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.7, stop_words='english')),\n",
       "                ('clf', OneVsRestClassifier(estimator=LogisticRegression()))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "\n",
    "#instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "#instantiate the classifier\n",
    "lr = LogisticRegression(random_state=1,  max_iter=1000,)\n",
    "\n",
    "#instantiate the pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', OneVsRestClassifier(lr))])\n",
    "\n",
    "#fit the model\n",
    "clf.fit(train['comment_text'], train[label_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9794619893684119"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions on the validation set\n",
    "preds = clf.predict_proba(valid['comment_text'])\n",
    "\n",
    "roc_auc_score(valid[label_cols], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating random-guess baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a copy of preds, \n",
    "random_guess = preds.copy()\n",
    "#set all values to 0\n",
    "random_guess[:]=0\n",
    "#set the first item in the array to 1 for each row\n",
    "random_guess[:,5]=1\n",
    "roc_auc_score(valid[label_cols], random_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#make a list of arrays that is the same length as the validation set\n",
    "random_guesses = [np.array([0.6]) for i in range(len(valid))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    31595\n",
       "1      319\n",
       "Name: severe_toxic, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid[label_cols[1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(valid[label_cols[0]],random_guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting test set to evaluate baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../data/test.csv.zip')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = clf.predict_proba(test['comment_text'])\n",
    "#put the predictions into test dataframe\n",
    "test[label_cols] = test_preds\n",
    "#drop the comment_text column\n",
    "test.drop('comment_text', axis=1, inplace=True)\n",
    "#save the test dataframe to a csv file\n",
    "test.to_csv('../data/submission1_LR.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.992855</td>\n",
       "      <td>0.124077</td>\n",
       "      <td>0.990072</td>\n",
       "      <td>0.021650</td>\n",
       "      <td>0.894809</td>\n",
       "      <td>0.158616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.013340</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.012149</td>\n",
       "      <td>0.004024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.046065</td>\n",
       "      <td>0.004860</td>\n",
       "      <td>0.018429</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.021711</td>\n",
       "      <td>0.005639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.001111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.040152</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.013592</td>\n",
       "      <td>0.003268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.992855      0.124077  0.990072  0.021650  0.894809   \n",
       "1  0000247867823ef7  0.013340      0.003690  0.008333  0.001904  0.012149   \n",
       "2  00013b17ad220c46  0.046065      0.004860  0.018429  0.001936  0.021711   \n",
       "3  00017563c3f7919a  0.003922      0.002305  0.003563  0.001111  0.003901   \n",
       "4  00017695ad8997eb  0.040152      0.002774  0.010028  0.002090  0.013592   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.158616  \n",
       "1       0.004024  \n",
       "2       0.005639  \n",
       "3       0.001111  \n",
       "4       0.003268  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(valid['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model runs 2 and 3: Random Forest with multiclass using OneVsRestClassifier, with max_df=1 and max_df=0.5\n",
    "\n",
    "Note: max_df=1 led to a steep decline in results whereas 0.5 got the same results as 0.7 in run 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=1, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000,\n",
       "                                                                  random_state=1)))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=1, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000,\n",
       "                                                                  random_state=1)))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=1, stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">clf: OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000, random_state=1))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=1)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=1, stop_words='english')),\n",
       "                ('clf',\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000,\n",
       "                                                                  random_state=1)))])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=1)\n",
    "\n",
    "#instantiate the classifier\n",
    "lr = LogisticRegression(random_state=1,  max_iter=1000,)\n",
    "\n",
    "#instantiate the pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', OneVsRestClassifier(lr))])\n",
    "\n",
    "#fit the model\n",
    "clf.fit(train['comment_text'], train[label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5492175306147505"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions on the validation set\n",
    "preds = clf.predict_proba(valid['comment_text'])\n",
    "\n",
    "roc_auc_score(valid[label_cols], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=0.5, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000,\n",
       "                                                                  random_state=1)))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=0.5, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000,\n",
       "                                                                  random_state=1)))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.5, stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">clf: OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000, random_state=1))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=1)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.5, stop_words='english')),\n",
       "                ('clf',\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000,\n",
       "                                                                  random_state=1)))])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.5)\n",
    "\n",
    "#instantiate the classifier\n",
    "lr = LogisticRegression(random_state=1,  max_iter=1000,)\n",
    "\n",
    "#instantiate the pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', OneVsRestClassifier(lr))])\n",
    "\n",
    "#fit the model\n",
    "clf.fit(train['comment_text'], train[label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9794619893684119"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions on the validation set\n",
    "preds = clf.predict_proba(valid['comment_text'])\n",
    "\n",
    "roc_auc_score(valid[label_cols], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv.zip')\n",
    "\n",
    "test_preds = clf.predict_proba(test['comment_text'])\n",
    "#put the predictions into test dataframe\n",
    "test[label_cols] = test_preds\n",
    "#drop the comment_text column\n",
    "test.drop('comment_text', axis=1, inplace=True)\n",
    "#save the test dataframe to a csv file\n",
    "test.to_csv('../data/submission3_LR.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 3: OVR with RF\n",
    "\n",
    "With a score of .933, It doesnt seem to do as well as LR-OVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=0.7, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 OneVsRestClassifier(estimator=RandomForestClassifier(max_depth=10,\n",
       "                                                                      random_state=1)))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=0.7, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 OneVsRestClassifier(estimator=RandomForestClassifier(max_depth=10,\n",
       "                                                                      random_state=1)))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.7, stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">clf: OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=RandomForestClassifier(max_depth=10,\n",
       "                                                     random_state=1))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, random_state=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, random_state=1)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.7, stop_words='english')),\n",
       "                ('clf',\n",
       "                 OneVsRestClassifier(estimator=RandomForestClassifier(max_depth=10,\n",
       "                                                                      random_state=1)))])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "#instantiate the random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=100, max_depth=10)\n",
    "\n",
    "#instantiate the pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', OneVsRestClassifier(rf))])\n",
    "\n",
    "#fit the model\n",
    "train_copy = train.copy() # create a copy of the train dataframe\n",
    "clf.fit(train_copy['comment_text'], train_copy[label_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9330115931912687"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions on the validation set\n",
    "preds = clf.predict_proba(valid['comment_text'])\n",
    "\n",
    "roc_auc_score(valid[label_cols], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attempt 2: Spacy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a one vs the rest spacy model\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#create a tokenizer function\n",
    "punctuations = string.punctuation\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "parser = English()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    #create token object, which is used to create documents with linguistic annotations\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    #lemmatize each token and convert each token into lowercase\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
    "\n",
    "    #remove stop words\n",
    "    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations]\n",
    "\n",
    "    #return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "#instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=spacy_tokenizer, max_df=0.7)\n",
    "\n",
    "#instantiate the random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=100, max_depth=10)\n",
    "\n",
    "#instantiate the pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', OneVsRestClassifier(rf))])\n",
    "\n",
    "#fit the model\n",
    "train_copy = train.copy() # create a copy of the train dataframe\n",
    "\n",
    "clf.fit(train_copy['comment_text'], train_copy[label_cols])\n",
    "\n",
    "#make predictions on the validation set\n",
    "preds = clf.predict_proba(valid['comment_text'])\n",
    "\n",
    "roc_auc_score(valid[label_cols], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using a spacy model for multilabel text classification\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import TextCategorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-dc86d62ab79d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"cats\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mtrain_examples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\spacy\\training\\example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.Example.from_dict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\spacy\\training\\example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.annotations_to_doc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\spacy\\training\\example.pyx\u001b[0m in \u001b[0;36mspacy.training.example._annot2array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline.textcat import Config, single_label_cnn_config\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Load a pre-trained SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define your text data and associated labels\n",
    "data = [\n",
    "    (\"This is a positive sentence.\", [\"positive\"]),\n",
    "    (\"This is another positive sentence.\", [\"positive\"]),\n",
    "    (\"This is a negative sentence.\", [\"negative\"]),\n",
    "    (\"This is a mixed sentence.\", [\"positive\", \"negative\"]),\n",
    "]\n",
    "\n",
    "# Split data into text and labels\n",
    "texts, labels = zip(*data)\n",
    "\n",
    "# Initialize a SpaCy text classifier with multi-label support\n",
    "config = Config().from_str(single_label_cnn_config)\n",
    "config['threshold'] = 0.5  # Adjust this threshold for label assignment\n",
    "\n",
    "text_classifier = nlp.add_pipe(\"textcat\", config=config, last=True)\n",
    "text_classifier.add_label(\"positive\")\n",
    "text_classifier.add_label(\"negative\")\n",
    "\n",
    "# Convert labels into a multi-label format\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_labels = mlb.fit_transform(labels)\n",
    "\n",
    "# Convert the entire training data into Example objects\n",
    "train_examples = []\n",
    "\n",
    "for text, annotations in zip(texts, binary_labels):\n",
    "    example = Example.from_dict(nlp.make_doc(text), {\"cats\": annotations})\n",
    "    train_examples.append(example)\n",
    "\n",
    "# Train the text classifier with the entire dataset\n",
    "for epoch in range(10):  # You can adjust the number of training epochs\n",
    "    losses = {}\n",
    "    random.shuffle(train_examples)\n",
    "    nlp.update(train_examples, drop=0.5, losses=losses)\n",
    "    print(losses)\n",
    "\n",
    "# Test the model on new text data\n",
    "test_data = [\"This is a positive sentence.\", \"This is a negative sentence.\", \"This is a mixed sentence.\"]\n",
    "for text in test_data:\n",
    "    doc = nlp(text)\n",
    "    predicted_labels = [label for label, score in doc.cats.items() if score >= 0.5]\n",
    "    print(f\"Text: {text}\\nPredicted Labels: {predicted_labels}\\n\")\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "# You can use metrics like accuracy, precision, recall, and F1-score for evaluation\n",
    "# Note that in multi-label classification, traditional accuracy may not be the best metric\n",
    "# You may want to look at precision, recall, and F1-score for each label separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigValidationError",
     "evalue": "\n\nConfig validation error\ntextcat -> architecture\textra fields not permitted\ntextcat -> exclusive_classes\textra fields not permitted\n{'nlp': <spacy.lang.en.English object at 0x0000024E65560FA0>, 'name': 'textcat', 'architecture': 'simple_cnn', 'exclusive_classes': False, 'model': {'@architectures': 'spacy.TextCatEnsemble.v2', 'linear_model': {'@architectures': 'spacy.TextCatBOW.v2', 'exclusive_classes': True, 'ngram_size': 1, 'no_output_layer': False}, 'tok2vec': {'@architectures': 'spacy.Tok2Vec.v2', 'embed': {'@architectures': 'spacy.MultiHashEmbed.v2', 'width': 64, 'rows': [2000, 2000, 1000, 1000, 1000, 1000], 'attrs': ['ORTH', 'LOWER', 'PREFIX', 'SUFFIX', 'SHAPE', 'ID'], 'include_static_vectors': False}, 'encode': {'@architectures': 'spacy.MaxoutWindowEncoder.v2', 'width': 64, 'window_size': 1, 'maxout_pieces': 3, 'depth': 2}}}, 'scorer': {'@scorers': 'spacy.textcat_scorer.v1'}, 'threshold': 0.5, '@factories': 'textcat'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConfigValidationError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-96d1f73b01ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#add the textcat component to the pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtextcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"textcat\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"exclusive_classes\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"architecture\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"simple_cnn\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextcat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[1;31m# We're calling the internal _fill here to avoid constructing the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;31m# registered functions twice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m         \u001b[0mresolved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"cfg\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cfg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\confection\\__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[1;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m     ) -> Dict[str, Any]:\n\u001b[1;32m--> 728\u001b[1;33m         resolved, _ = cls._make(\n\u001b[0m\u001b[0;32m    729\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\confection\\__init__.py\u001b[0m in \u001b[0;36m_make\u001b[1;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_interpolated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m         filled, _, resolved = cls._fill(\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\confection\\__init__.py\u001b[0m in \u001b[0;36m_fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    830\u001b[0m                     \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy_model_field\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m                 \u001b[0mpromise_schema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_promise_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 832\u001b[1;33m                 filled[key], validation[v_key], final[key] = cls._fill(\n\u001b[0m\u001b[0;32m    833\u001b[0m                     \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m                     \u001b[0mpromise_schema\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mlar5\\anaconda3\\envs\\TextMining\\lib\\site-packages\\confection\\__init__.py\u001b[0m in \u001b[0;36m_fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    896\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m                 raise ConfigValidationError(\n\u001b[0m\u001b[0;32m    899\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m                 ) from None\n",
      "\u001b[1;31mConfigValidationError\u001b[0m: \n\nConfig validation error\ntextcat -> architecture\textra fields not permitted\ntextcat -> exclusive_classes\textra fields not permitted\n{'nlp': <spacy.lang.en.English object at 0x0000024E65560FA0>, 'name': 'textcat', 'architecture': 'simple_cnn', 'exclusive_classes': False, 'model': {'@architectures': 'spacy.TextCatEnsemble.v2', 'linear_model': {'@architectures': 'spacy.TextCatBOW.v2', 'exclusive_classes': True, 'ngram_size': 1, 'no_output_layer': False}, 'tok2vec': {'@architectures': 'spacy.Tok2Vec.v2', 'embed': {'@architectures': 'spacy.MultiHashEmbed.v2', 'width': 64, 'rows': [2000, 2000, 1000, 1000, 1000, 1000], 'attrs': ['ORTH', 'LOWER', 'PREFIX', 'SUFFIX', 'SHAPE', 'ID'], 'include_static_vectors': False}, 'encode': {'@architectures': 'spacy.MaxoutWindowEncoder.v2', 'width': 64, 'window_size': 1, 'maxout_pieces': 3, 'depth': 2}}}, 'scorer': {'@scorers': 'spacy.textcat_scorer.v1'}, 'threshold': 0.5, '@factories': 'textcat'}"
     ]
    }
   ],
   "source": [
    "#load the spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#add the textcat component to the pipeline\n",
    "textcat = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": False, \"architecture\": \"simple_cnn\"})\n",
    "nlp.add_pipe(textcat, last=True)\n",
    "\n",
    "#add the labels to the textcat component\n",
    "textcat.add_label('toxic')\n",
    "textcat.add_label('severe_toxic')\n",
    "textcat.add_label('obscene')\n",
    "textcat.add_label('threat')\n",
    "textcat.add_label('insult')\n",
    "textcat.add_label('identity_hate')\n",
    "textcat.add_label('non-toxic')\n",
    "\n",
    "#convert the train data into a list of tuples\n",
    "train_data = list(train.apply(lambda x: (x['comment_text'], {'cats': {'toxic': x['toxic'], 'severe_toxic': x['severe_toxic'], 'obscene': x['obscene'], 'threat': x['threat'], 'insult': x['insult'], 'identity_hate': x['identity_hate'], 'non-toxic': x['non-toxic']}}), axis=1))\n",
    "\n",
    "#convert the validation data into a list of tuples\n",
    "valid_data = list(valid.apply(lambda x: (x['comment_text'], {'cats': {'toxic': x['toxic'], 'severe_toxic': x['severe_toxic'], 'obscene': x['obscene'], 'threat': x['threat'], 'insult': x['insult'], 'identity_hate': x['identity_hate'], 'non-toxic': x['non-toxic']}}), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'dateutil'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "\n",
    "#convert the test data into a list of tuples\n",
    "#test_data = list(test.apply(lambda x: (x['comment_text'], {'cats': {'toxic': x['toxic'], 'severe_toxic': x['severe_toxic'], 'obscene': x['obscene'], 'threat': x['threat'], 'insult': x['insult'], 'identity_hate': x['identity_hate'], 'non-toxic': x['non-toxic']}}), axis=1))\n",
    "\n",
    "#train the model\n",
    "n_iter=10\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    print(\"Training model...\")\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'dateutil'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "def evaluate(model, data):\n",
    "    scorer = nlp.evaluate(data)\n",
    "    print('Losses', scorer['textcat_loss'])\n",
    "    print('Accuracy', scorer['textcat_acc'])\n",
    "    return scorer\n",
    "\n",
    "evaluate(nlp, valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3: huggingface\n",
    "\n",
    "After doing some research, it is clear that we can also use huggingface to train a model, outputting the probabilities and then performing auc-roc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_assertion_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
