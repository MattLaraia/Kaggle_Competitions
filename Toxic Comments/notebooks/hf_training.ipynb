{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2: huggingface transformers\n",
    "\n",
    "After doing some research, it is clear that we can also use huggingface to train a model, outputting the probabilities and then performing auc-roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons learned\n",
    "\n",
    "Found that the model was training slow when training with a greater batch size than 8 despite knowing that I had used batch size 32 and 64 on this same machine with a similiar dataset size.  \n",
    "\n",
    "What I failed to initially realize was that the tokenized data was of a larger token max_length than the previous dataset.  The other set had max_length padding of size 128 while here I was using 512 since the model was bert-base-uncased.  Since the padding was 4x larger, the batch size had to be 4x smaller to fit into the GPU memory. \n",
    "\n",
    "# Issues\n",
    "The subtrain/subval split is not using the same split because i stratify by each of the individual labels. This is not the same as stratifying by the combination of labels, giving different data to be used on the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUN=1\n",
    "#load csv files\n",
    "train = pd.read_csv('../data/train_split.csv')\n",
    "valid = pd.read_csv('../data/valid_split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic',\t'severe_toxic'\t,'obscene',\t'threat',\t'insult', \t'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dfs_train = {}\n",
    "for label in label_cols:\n",
    "    train_copy = train.copy()\n",
    "    #stratify binary_dfs_train['toxic'] with train as 3% of the data\n",
    "    subtrain, _ = train_test_split(train_copy, test_size=0.97, random_state=42, stratify=train_copy[label], )\n",
    "    binary_dfs_train[label] = subtrain.copy()\n",
    "    binary_dfs_train[label]['label_name'] = binary_dfs_train[label][label].apply(lambda x: label if x==1 else 'other')\n",
    "    binary_dfs_train[label] = binary_dfs_train[label][['id', 'comment_text', label]]\n",
    "    #rename the label column to 'label'\n",
    "    binary_dfs_train[label].rename(columns={label:'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dfs_train['severe_toxic']['text_word_count'] = binary_dfs_train['severe_toxic']['comment_text'].apply(lambda x: len(x.split()))\n",
    "binary_dfs_train['severe_toxic'][binary_dfs_train['severe_toxic']['text_word_count']>500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dfs_val = {}\n",
    "for label in label_cols:\n",
    "    val_copy = valid.copy()\n",
    "    #stratify binary_dfs_val['toxic'] with 10% of the data\n",
    "    subvalid,_ = train_test_split(val_copy, test_size=0.9, random_state=42, stratify=val_copy[label] )\n",
    "    binary_dfs_val[label] = subvalid.copy()\n",
    "    binary_dfs_val[label]['label_name'] = binary_dfs_val[label][label].apply(lambda x: label if x==1 else 'other')\n",
    "    binary_dfs_val[label] = binary_dfs_val[label][['id', 'comment_text', label]]\n",
    "    #rename the label column to 'label'\n",
    "    binary_dfs_val[label].rename(columns={label:'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#train a huggingface classifier\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if cuda is available\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B disabled.\n"
     ]
    }
   ],
   "source": [
    "!wandb disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1380 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 9723.16 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 9754.20 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 9716.55 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 9545.50 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 10024.99 examples/s]\n",
      "Map: 100%|██████████| 3829/3829 [00:00<00:00, 9205.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "binary_datasets_train = {}\n",
    "for key in binary_dfs_train.keys():\n",
    "    \n",
    "    #convert the data into a Dataset object\n",
    "    binary_datasets_train[key] = datasets.Dataset.from_pandas(binary_dfs_train[key])\n",
    "    binary_datasets_train[key] = binary_datasets_train[key].map(lambda batch: tokenizer(batch['comment_text'], truncation=True, padding='max_length',max_length=128), batched=True,batch_size=64)\n",
    "    \n",
    "    binary_datasets_train[key].set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7194.52 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7588.55 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7322.71 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7109.12 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7199.08 examples/s]\n",
      "Map: 100%|██████████| 3191/3191 [00:00<00:00, 7710.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "binary_datasets_val = {}\n",
    "for key in binary_dfs_val.keys():\n",
    "\n",
    "\n",
    "    #convert the data into a Dataset object\n",
    "    binary_datasets_val[key] = datasets.Dataset.from_pandas(binary_dfs_val[key])\n",
    "    binary_datasets_val[key] = binary_datasets_val[key].map(lambda batch: tokenizer(batch['comment_text'], truncation=True, padding='max_length',max_length=128), batched=True,batch_size=64)\n",
    "    \n",
    "    binary_datasets_val[key].set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      " 22%|██▏       | 80/360 [00:13<00:45,  6.15it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mlaraia\\extra_projects\\Kaggle_Competitions\\Toxic Comments\\notebooks\\hf_training.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,                         \u001b[39m# the instantiated 🤗 Transformers model to be trained\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,                  \u001b[39m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mbinary_datasets_train[key],         \u001b[39m# training dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mbinary_datasets_val[key],             \u001b[39m# evaluation dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#evaluate the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mlaraia/extra_projects/Kaggle_Competitions/Toxic%20Comments/notebooks/hf_training.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mlaraia\\anaconda3\\envs\\test1\\lib\\site-packages\\transformers\\trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1836\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1837\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1840\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1841\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1842\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1843\u001b[0m ):\n\u001b[0;32m   1844\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1845\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import trainer and training arguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "for key in binary_datasets_train.keys():\n",
    "    #instantiate the model and use the label list to define the labels\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    #instantiate the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'../results/{key}/{RUN}',          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=64,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "        learning_rate=5e-05,             # learning rate\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    #instantiate the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=binary_datasets_train[key],         # training dataset\n",
    "        eval_dataset=binary_datasets_val[key],             # evaluation dataset\n",
    "    )   \n",
    "\n",
    "    #train the model\n",
    "    trainer.train()\n",
    "\n",
    "    #evaluate the model\n",
    "    trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
